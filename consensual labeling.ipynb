{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bbe2b15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832eb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from rapidfuzz import fuzz\n",
    "import textwrap\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdeb71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0696fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CORE CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Model & Path Configuration ---\n",
    "# NOTE: Replace with the actual paths to your fine-tuned model adapters\n",
    "MODEL_PATHS = {\n",
    "    \"gemma_ft_1\": \"./lora_finetuned_model_1\",\n",
    "    \"gemma_ft_2\": \"./lora_finetuned_model_2\",\n",
    "    \"gemma_ft_3\": \"./lora_finetuned_model_3\"\n",
    "}\n",
    "BASE_MODEL_NAME = \"google/gemma-3-4b-it\"\n",
    "\n",
    "# --- Trustworthiness & Consensus Configuration ---\n",
    "INITIAL_TRUST_SCORE = 1.0\n",
    "MIN_TRUST_SCORE = 0.5\n",
    "MAX_TRUST_SCORE = 2.0\n",
    "CONFIDENT_SCORE_ADJUSTMENT = 0.1\n",
    "PROBABLE_SCORE_ADJUSTMENT = 0.05\n",
    "\n",
    "TRUST_TIERS = {\n",
    "    \"Trustworthy\": 1.5,\n",
    "    \"Mid-tier\": 0.8,\n",
    "    \"Untrustworthy\": MIN_TRUST_SCORE\n",
    "}\n",
    "SIMILARITY_THRESHOLD = 85 # For clustering 'valor' text\n",
    "\n",
    "# --- Master List of All Possible Fields and Classes ---\n",
    "ALL_CLASSES = ['S', 'D', 'M', 'DS', 'PM', 'SV', 'R', 'P']\n",
    "ALL_FIELDS = ['id', 'class', 'valor', 'estado_clinico', 'loc-temp', 'quem', 'relacionamento_id']\n",
    "\n",
    "# --- Prompt Instructions for the Models ---\n",
    "INSTRUCTIONS = textwrap.dedent(\"\"\"\n",
    "VocÃª Ã© um assistente especializado em Reconhecimento de Entidades Nomeadas (NER) na Ã¡rea mÃ©dica.\n",
    "Seu objetivo Ã© identificar e extrair termos relevantes de um diÃ¡rio mÃ©dico de cardiologia.\n",
    "Formate a saÃ­da como uma lista JSON vÃ¡lida, onde cada objeto representa uma entidade identificada.\n",
    "\n",
    "InstruÃ§Ãµes para o formato JSON:\n",
    "1. A saÃ­da DEVE ser uma lista JSON `[...]`.\n",
    "2. Cada objeto na lista deve ter as seguintes chaves:\n",
    "    - `id`: (Inteiro) ID numÃ©rico sequencial da entidade (1, 2, 3...).\n",
    "    - `classe`: (String) A classe da entidade (use os cÃ³digos: S, D, M, DS, PM, SV, R, P).\n",
    "    - `valor`: (String) O texto exato da entidade identificada.\n",
    "    - `estado_clinico`: (String) No caso de um diagnÃ³stico ou sintoma, indica se a pessoa tem essa condiÃ§Ã£o (\"+\"), se nÃ£o tem (\"-\") ou se Ã© uma possibilidade (\"?\"). Omita se nÃ£o aplicÃ¡vel.\n",
    "    - `loc-temp`: (String) O valor de localizaÃ§Ã£o temporal deve estar presente apenas caso nÃ£o seja o momento atual (hoje) (ex: \"ontem\", \"2014\", \"na infÃ¢ncia\", \"hÃ¡ 3 dias\"). Omita se nÃ£o aplicÃ¡vel.\n",
    "    - `quem`: (String) O valor de quem tem uma condiÃ§Ã£o, diagnÃ³stico, medicaÃ§Ã£o, etc., presente apenas no caso de nÃ£o se referir ao prÃ³prio paciente (ex: \"familiar(filho, mÃ£e, etc.)\"). Omita se nÃ£o aplicÃ¡vel.\n",
    "    - `relacionamento_id`: (Inteiro) Inclua esta chave *apenas* quando 2 classes estÃ£o relacionadas, indicando o `id` da entidade relacionada (ex: o `id` do Medicamento para uma Dosagem). Omita esta chave noutros casos.\n",
    "3. Certifique-se de que a saÃ­da final seja um JSON estritamente vÃ¡lido. NÃ£o inclua nenhum texto antes ou depois da lista JSON, nem marcadores como ```json.\n",
    "\n",
    "Classes permitidas e seus significados:\n",
    "    - S: Sintoma\n",
    "    - D: DiagnÃ³stico\n",
    "    - M: Medicamento\n",
    "    - DS: Dosagem\n",
    "    - PM: Procedimento MÃ©dico\n",
    "    - SV: Sinal Vital\n",
    "    - R: Resultado (de exame, etc.)\n",
    "    - P: Progresso (do paciente)\n",
    "\n",
    "Exemplos de SaÃ­da JSON VÃ¡lida:\n",
    "[\n",
    "  {\"id\": 1, \"classe\": \"S\", \"valor\": \"Dor torÃ¡cica\", \"estado_clinico\": \"+\", \"loc-temp\": \"2015\", \"quem\": \"mÃ£e\"},\n",
    "  {\"id\": 2, \"classe\": \"M\", \"valor\": \"Aspirina\", \"loc-temp\": \"ontem\"},\n",
    "  {\"id\": 3, \"classe\": \"D\", \"valor\": \"HipertensÃ£o\", \"estado_clinico\": \"-\",\"quem\": \"familiar\"},\n",
    "  {\"id\": 4, \"classe\": \"DS\", \"valor\": \"100\", \"relacionamento_id\": 2},\n",
    "  {\"id\": 5, \"classe\": \"SV\", \"valor\": \"PA 120/80 mmHg\"},\n",
    "  {\"id\": 6, \"classe\": \"PM\", \"valor\": \"Exame sangue\"},\n",
    "  {\"id\": 7, \"classe\": \"R\", \"valor\": \"sangue normal\", \"relacionamento_id\": 6}\n",
    "]\n",
    "\"\"\").strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77261916",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# --- 2. MODEL AND DATA LOADING ---\n",
    "# ==============================================================================\n",
    "\n",
    "def load_all_models(model_paths, base_model_name):\n",
    "    \"\"\"Loads all fine-tuned models from the specified paths.\"\"\"\n",
    "    models = {}\n",
    "    for name, path in model_paths.items():\n",
    "        print(f\"Loading model '{name}' from {path}...\")\n",
    "        try:\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(base_model_name, attn_implementation=\"eager\")\n",
    "            model = PeftModel.from_pretrained(base_model, path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "            models[name] = (model, tokenizer)\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Could not load model '{name}'. Skipping. Error: {e}\")\n",
    "    return models\n",
    "\n",
    "def load_gold_standard_data(file_path):\n",
    "    \"\"\"Loads the gold standard JSON data for the preparation phase.\"\"\"\n",
    "    print(f\"Loading gold standard data from '{file_path}'...\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  [ERROR] Gold standard file not found at '{file_path}'. Cannot run preparation phase.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  [ERROR] Gold standard file '{file_path}' is not valid JSON.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c14d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# --- 3. PREDICTION AND PREPARATION PHASE ---\n",
    "# ==============================================================================\n",
    "\n",
    "def get_predictions_from_model(text, model, tokenizer, instructions):\n",
    "    \"\"\"Generates structured JSON predictions from a single model.\"\"\"\n",
    "    prompt = f\"{instructions}\\n\\nDiÃ¡rio MÃ©dico para AnÃ¡lise:\\n---\\n{text}\\n---\\n\\nSaÃ­da JSON:\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        json_str_match = re.search(r'\\[.*\\]', decoded_output, re.DOTALL)\n",
    "        if json_str_match:\n",
    "            json_str = json_str_match.group(0)\n",
    "            return json.loads(json_str)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Model prediction failed or produced invalid JSON. Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def run_preparation_phase(models, gold_standard_data, instructions):\n",
    "    \"\"\"Calculates initial, granular trust scores for all models based on a gold standard dataset.\"\"\"\n",
    "    print(\"\\nðŸš€ Starting Preparation Phase...\")\n",
    "    \n",
    "    correct_counts = {m: defaultdict(lambda: defaultdict(int)) for m in models}\n",
    "    total_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i, entry in enumerate(gold_standard_data, 1):\n",
    "        print(f\"  Processing gold standard entry {i}/{len(gold_standard_data)}...\")\n",
    "        text = entry[\"text\"]\n",
    "        gold_labels = entry[\"labels\"]\n",
    "\n",
    "        # Update total counts for each field in the gold standard\n",
    "        for gold_label in gold_labels:\n",
    "            for field, value in gold_label.items():\n",
    "                total_counts[gold_label['class']][field] += 1\n",
    "\n",
    "        for model_name, (model, tokenizer) in models.items():\n",
    "            model_predictions = get_predictions_from_model(text, model, tokenizer, instructions)\n",
    "            \n",
    "            # Compare model predictions against gold labels field by field\n",
    "            for gold_label in gold_labels:\n",
    "                for model_pred in model_predictions:\n",
    "                    if gold_label['class'] == model_pred['class'] and fuzz.ratio(gold_label['valor'], model_pred['valor']) > 95:\n",
    "                        # Found a matching entity, now check each field\n",
    "                        for field, gold_value in gold_label.items():\n",
    "                            if str(model_pred.get(field)) == str(gold_value):\n",
    "                                correct_counts[model_name][gold_label['class']][field] += 1\n",
    "                        break\n",
    "\n",
    "    # Calculate and store the initial trust scores\n",
    "    initial_trust_scores = {m: defaultdict(dict) for m in models}\n",
    "    for model_name in models:\n",
    "        for class_name, fields in total_counts.items():\n",
    "            for field, total in fields.items():\n",
    "                accuracy = correct_counts[model_name][class_name][field] / total if total > 0 else 0\n",
    "                # Scale accuracy to our score range and clamp it\n",
    "                score = INITIAL_TRUST_SCORE + (accuracy - 0.5) * 2 \n",
    "                initial_trust_scores[model_name][class_name][field] = max(MIN_TRUST_SCORE, min(score, MAX_TRUST_SCORE))\n",
    "\n",
    "    print(\"âœ… Preparation Phase Complete.\")\n",
    "    return initial_trust_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d621198",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5217000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. ADVANCED CONSENSUS LOGIC ---\n",
    "# ==============================================================================\n",
    "\n",
    "def get_field_consensus(cluster, class_name, field_name, trust_scores):\n",
    "    \"\"\"Determines the consensus value for a single field within a cluster.\"\"\"\n",
    "    \n",
    "    def get_trusty_prediction():\n",
    "        \"\"\"Finds the prediction from the most trustworthy model for this specific task.\"\"\"\n",
    "        best_model = max(cluster, key=lambda p: trust_scores.get(p['model'], {}).get(class_name, {}).get(field_name, INITIAL_TRUST_SCORE))\n",
    "        return best_model.get(field_name, \"OMIT\")\n",
    "\n",
    "    votes = Counter(p.get(field_name, \"OMIT\") for p in cluster)\n",
    "    most_common = votes.most_common(2)\n",
    "\n",
    "    if len(most_common) == 1 or (len(most_common) > 1 and most_common[0][1] > most_common[1][1]):\n",
    "        consensus = most_common[0][0]\n",
    "    else: # Tie-breaker\n",
    "        consensus = get_trusty_prediction()\n",
    "        \n",
    "    return consensus if consensus != \"OMIT\" else None\n",
    "\n",
    "def get_tiered_consensus(text, models, instructions, trust_scores):\n",
    "    \"\"\"Orchestrates the entire consensus process for a single piece of text.\"\"\"\n",
    "    \n",
    "    # 1. Get Predictions from all models\n",
    "    all_predictions = []\n",
    "    for model_name, (model, tokenizer) in models.items():\n",
    "        preds = get_predictions_from_model(text, model, tokenizer, instructions)\n",
    "        for p in preds:\n",
    "            p['model'] = model_name # Tag prediction with its source model\n",
    "        all_predictions.extend(preds)\n",
    "\n",
    "    # 2. Cluster predictions by class and valor similarity\n",
    "    clusters = []\n",
    "    for pred in all_predictions:\n",
    "        found_cluster = False\n",
    "        for cluster in clusters:\n",
    "            if pred['class'] == cluster[0]['class'] and fuzz.ratio(pred['valor'], cluster[0]['valor']) > SIMILARITY_THRESHOLD:\n",
    "                cluster.append(pred)\n",
    "                found_cluster = True\n",
    "                break\n",
    "        if not found_cluster:\n",
    "            clusters.append([pred])\n",
    "\n",
    "    # 3. Build consensus for each cluster\n",
    "    final_entities = []\n",
    "    for cluster in clusters:\n",
    "        class_name = cluster[0]['class']\n",
    "        \n",
    "        # Build consensus for each field\n",
    "        consensus_obj = {\"class\": class_name}\n",
    "        for field in ALL_FIELDS:\n",
    "            if field in ['id', 'class']: continue # Skip these\n",
    "            \n",
    "            consensus_value = get_field_consensus(cluster, class_name, field, trust_scores)\n",
    "            if consensus_value is not None:\n",
    "                consensus_obj[field] = consensus_value\n",
    "        \n",
    "        # 4. Determine Confidence Tier\n",
    "        agreeing_models = {p['model'] for p in cluster if p['valor'] == consensus_obj['valor']}\n",
    "        disagreeing_model_count = len(models) - len(agreeing_models)\n",
    "        \n",
    "        confidence_tier = \"Needs Human Review\" # Default\n",
    "        if disagreeing_model_count == 0:\n",
    "            confidence_tier = \"Confident\"\n",
    "        elif disagreeing_model_count == 1:\n",
    "            confidence_tier = \"Probable\"\n",
    "            \n",
    "        consensus_obj[\"confidence\"] = confidence_tier\n",
    "        final_entities.append(consensus_obj)\n",
    "\n",
    "        # 5. Update Trust Scores (Conditional & Tiered)\n",
    "        if confidence_tier in [\"Confident\", \"Probable\"]:\n",
    "            increment = CONFIDENT_SCORE_ADJUSTMENT if confidence_tier == \"Confident\" else PROBABLE_SCORE_ADJUSTMENT\n",
    "            \n",
    "            for model_name in models:\n",
    "                model_pred = next((p for p in cluster if p['model'] == model_name), None)\n",
    "                \n",
    "                for field in ALL_FIELDS:\n",
    "                    if field == 'id': continue\n",
    "\n",
    "                    current_score = trust_scores[model_name].get(class_name, {}).get(field, INITIAL_TRUST_SCORE)\n",
    "                    \n",
    "                    if model_pred and str(model_pred.get(field)) == str(consensus_obj.get(field)):\n",
    "                        new_score = current_score + increment\n",
    "                    else:\n",
    "                        new_score = current_score - increment\n",
    "                    \n",
    "                    # Clamp the score\n",
    "                    trust_scores[model_name][field] = max(MIN_TRUST_SCORE, min(new_score, MAX_TRUST_SCORE))\n",
    "\n",
    "    # Assign final sequential IDs\n",
    "    for i, entity in enumerate(final_entities, 1):\n",
    "        entity['id'] = i\n",
    "        \n",
    "    return final_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f459c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. MAIN EXECUTION SCRIPT ---\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire pipeline.\"\"\"\n",
    "    \n",
    "    # --- Load Models ---\n",
    "    loaded_models = load_all_models(MODEL_PATHS, BASE_MODEL_NAME)\n",
    "    if not loaded_models:\n",
    "        print(\"No models were loaded. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Run Preparation Phase ---\n",
    "    gold_data = load_gold_standard_data(\"gold_standard_diaries.json\")\n",
    "    if gold_data:\n",
    "        trust_scores = run_preparation_phase(loaded_models, gold_data, INSTRUCTIONS)\n",
    "        print(\"\\n--- Initial Trust Scores ---\")\n",
    "        print(json.dumps(trust_scores, indent=2))\n",
    "    else:\n",
    "        # Initialize with default scores if no gold data\n",
    "        trust_scores = {m: {c: {f: INITIAL_TRUST_SCORE for f in ALL_FIELDS} for c in ALL_CLASSES} for m in loaded_models}\n",
    "        print(\"\\n--- Initializing with default trust scores ---\")\n",
    "\n",
    "\n",
    "    # --- Process New Unlabeled Diaries ---\n",
    "    unlabeled_file = \"diarios-cardiologia-amostra.txt\" # or \"diarios-psicologia-amostra.txt\"\n",
    "    print(f\"\\n\\nðŸš€ Starting Labeling Phase for '{unlabeled_file}'...\")\n",
    "    \n",
    "    try:\n",
    "        with open(unlabeled_file, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "            # This processes the entire file as one document. \n",
    "            # You can adapt this to process entry by entry as before.\n",
    "            final_consensus_labels = get_tiered_consensus(full_text, loaded_models, INSTRUCTIONS, trust_scores)\n",
    "            \n",
    "            print(\"\\n\\nâœ… --- FINAL CONSENSUS LABELS --- âœ…\")\n",
    "            print(json.dumps(final_consensus_labels, indent=2, ensure_ascii=False))\n",
    "            \n",
    "            print(\"\\n\\n--- Final Trust Scores ---\")\n",
    "            print(json.dumps(trust_scores, indent=2))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  [ERROR] Unlabeled diary file not found at: '{unlabeled_file}'\")\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"\\nCleaning up resources...\")\n",
    "    del loaded_models\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Script finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NOTE: To run this script, you must have:\n",
    "    # 1. Your fine-tuned model directories at the paths specified in MODEL_PATHS.\n",
    "    # 2. A 'gold_standard_diaries.json' file for the preparation phase.\n",
    "    # 3. The unlabeled diary file you wish to process.\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Prologica_new (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
