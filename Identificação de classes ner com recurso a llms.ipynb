{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Entidades NER e Parâmetros Clínicos com LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script foi concebido para realizar o Reconhecimento de Entidades Nomeadas (NER) em diários clínicos de cardiologia. Utilizando o poder de um modelo de linguagem avançado (LLM), especificamente o google/gemma-3-12b-it, o script processa texto não estruturado para identificar, extrair e classificar informações cruciais. As entidades médicas, como sintomas, diagnósticos, medicamentos e sinais vitais, são extraídas e estruturadas num formato JSON claro e válido. O sistema é flexível, permitindo a execução tanto em CPU, através da biblioteca llama-cpp-python, como em GPUs potentes, com o auxílio da biblioteca transformers da Hugging Face, tornando a análise de dados clínicos mais eficiente e automatizada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a08f9d",
   "metadata": {},
   "source": [
    "###  Importações e Configurações Globais\n",
    "Este bloco de código inicial é responsável por configurar o ambiente de execução do script. Primeiramente, importa as bibliotecas essenciais como os, re e sys para manipulação do sistema e expressões regulares. Em seguida, define um interruptor crucial, USE_LLAMA_CPP, que permite ao utilizador escolher entre dois backends de inferência: llama-cpp-python para uma execução otimizada em CPU, ou a biblioteca transformers da Hugging Face, que é mais adequada para ambientes com GPUs potentes. O código também realiza importações condicionais com base nesta escolha e estabelece parâmetros globais que irão governar o comportamento do modelo, como o tamanho máximo do contexto (N_CTX), o número de threads (N_THREADS) e os parâmetros de geração de texto (MAX_TOKENS, TEMPERATURE, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "# --- Interruptor de Backend ---\n",
    "# Altere esta variável para escolher o motor de inferência.\n",
    "# True: Usa llama-cpp-python (otimizado para CPU, mais rápido sem GPU).\n",
    "# False: Usa a biblioteca transformers da Hugging Face (requer GPU potente para este modelo).\n",
    "USE_LLAMA_CPP = True\n",
    "\n",
    "# --- Importações Condicionais ---\n",
    "if USE_LLAMA_CPP:\n",
    "    from llama_cpp import Llama\n",
    "else:\n",
    "    try:\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "    except ImportError:\n",
    "        print(\"Erro: A biblioteca 'transformers' e/ou 'torch' não estão instaladas.\")\n",
    "        print(\"Para usar o backend da Hugging Face, instale-as com: pip install transformers torch\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# --- Configurações Globais ---\n",
    "# Parâmetros para a execução do modelo e processamento de texto.\n",
    "N_CTX = 131072\n",
    "N_THREADS = 16  # Relevante principalmente para llama-cpp\n",
    "MAX_TOKENS = 4096\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 0.95\n",
    "TOP_K = 60\n",
    "MIN_P = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95c3ef49",
   "metadata": {},
   "source": [
    "### Inicialização do Modelo (Modularizada)\n",
    "A função initialize_model encapsula a lógica de carregamento do modelo de linguagem. Dependendo do valor da variável USE_LLAMA_CPP, esta função segue um de dois caminhos. Se USE_LLAMA_CPP for verdadeiro, ela descarrega e inicializa o modelo gemma-3-12b-it numa versão otimizada para CPU (formato GGUF), configurando-o para forçar a computação em CPU (n_gpu_layers=0). Caso contrário, o script tenta carregar o modelo completo através da biblioteca transformers, que requer PyTorch e uma GPU com memória de vídeo substancial (>24GB). A função inclui gestão de erros robusta para lidar com falhas durante o processo de inicialização e informa o utilizador sobre qual backend está a ser utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inicialização do Modelo (Modularizada) ---\n",
    "def initialize_model():\n",
    "    \"\"\"\n",
    "    Inicializa e carrega o modelo e o tokenizer com base no backend selecionado.\n",
    "    \"\"\"\n",
    "    print(f\"A inicializar o modelo com o backend: {'llama-cpp-python' if USE_LLAMA_CPP else 'Hugging Face Transformers'}...\")\n",
    "\n",
    "    if USE_LLAMA_CPP:\n",
    "        try:\n",
    "            model = Llama.from_pretrained(\n",
    "                repo_id=\"google/gemma-3-12b-it-qat-q4_0-gguf\",\n",
    "                filename=\"gemma-3-12b-it-q4_0.gguf\",\n",
    "                n_ctx=N_CTX,\n",
    "                n_threads=N_THREADS,\n",
    "                n_gpu_layers=0,  # Forçar CPU\n",
    "                verbose=False\n",
    "            )\n",
    "            tokenizer = None  # llama-cpp lida com a tokenização internamente\n",
    "            print(\"Modelo Llama.cpp inicializado com sucesso.\")\n",
    "            return model, tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao inicializar o modelo Llama.cpp: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        # --- AVISO: Requer GPU com >24GB de VRAM para o modelo 12B ---\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"AVISO: Nenhuma GPU detetada. O backend 'transformers' será extremamente lento em CPU.\")\n",
    "\n",
    "        try:\n",
    "            model_name = \"google/gemma-3-12b-it\"\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",  # Tenta usar GPU se disponível\n",
    "                torch_dtype=torch.bfloat16, # Otimização para GPUs mais recentes\n",
    "                attn_implementation=\"flash_attention_2\" # Requer hardware compatível\n",
    "            )\n",
    "            print(\"Modelo Hugging Face Transformers inicializado com sucesso.\")\n",
    "            return model, tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao inicializar o modelo Hugging Face: {e}\")\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f866982b",
   "metadata": {},
   "source": [
    "### Função de Geração de NER\n",
    "A função generate_ner_json_streaming é o núcleo da lógica de análise. Ela recebe o texto de um diário clínico e constrói um prompt detalhado para o modelo de linguagem. Este prompt instrui o modelo a agir como um especialista em NER médico e a extrair entidades específicas (Sintomas, Diagnósticos, Medicamentos, etc.). As instruções são muito precisas, definindo um formato de saída JSON estrito, incluindo as chaves esperadas (id, classe, valor, etc.) e as regras para o seu preenchimento. A função envia o prompt para o modelo carregado e processa a resposta em streaming, imprimindo o JSON gerado em tempo real. Esta abordagem é eficiente e fornece feedback imediato ao utilizador.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função de Geração de NER ---\n",
    "def generate_ner_json_streaming(text: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Gera entidades NER utilizando o backend configurado.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto do diário médico a ser analisado.\n",
    "        model: A instância do modelo carregado.\n",
    "        tokenizer: O tokenizer correspondente (ou None para llama-cpp).\n",
    "    \"\"\"\n",
    "    instructions = textwrap.dedent(\"\"\"\n",
    "    És um assistente especializado em Reconhecimento de Entidades Nomeadas (NER) na área médica.\n",
    "    O teu objetivo é identificar e extrair termos relevantes de um diário médico de cardiologia.\n",
    "    Formata a saída como uma lista JSON válida, onde cada objeto representa uma entidade identificada.\n",
    "\n",
    "    Instruções para o formato JSON:\n",
    "    1. A saída DEVE ser uma lista JSON `[...]`.\n",
    "    2. Cada objeto na lista deve ter as seguintes chaves:\n",
    "        - `id`: (Inteiro) ID numérico sequencial da entidade (1, 2, 3...).\n",
    "        - `classe`: (String) A classe da entidade (use os códigos: S, D, M, DS, PM, SV, R, P).\n",
    "        - `valor`: (String) O texto exato da entidade identificada.\n",
    "        - `estado_clinico`: (String) No caso de um diagnóstico ou sintoma, indica se a pessoa tem essa condição (\"+\"), se não tem (\"-\") ou se é uma possibilidade (\"?\"). Omita se não aplicável.\n",
    "        - `loc-temp`: (String) O valor de localização temporal deve estar presente apenas caso não seja o momento atual (hoje) (ex: \"ontem\", \"2014\", \"na infância\", \"há 3 dias\"). Omita se não aplicável.\n",
    "        - `quem`: (String) O valor de quem tem uma condição, diagnóstico, medicação, etc., presente apenas no caso de não se referir ao próprio paciente (ex: \"familiar(filho, mãe, etc.)\"). Omita se não aplicável.\n",
    "        - `relacionamento_id`: (Inteiro) Inclua esta chave *apenas* quando 2 classes estão relacionadas, indicando o `id` da entidade relacionada (ex: o `id` do Medicamento para uma Dosagem). Omita esta chave noutros casos.\n",
    "    3. Certifique-se de que a saída final seja um JSON estritamente válido. Não inclua nenhum texto antes ou depois da lista JSON, nem marcadores como ```json.\n",
    "\n",
    "    Classes permitidas e seus significados:\n",
    "        - S: Sintoma, D: Diagnóstico, M: Medicamento, DS: Dosagem, PM: Procedimento Médico, SV: Sinal Vital, R: Resultado, P: Progresso\n",
    "\n",
    "    Exemplo de Saída JSON Válida:\n",
    "    [\n",
    "      {\"id\": 1, \"classe\": \"S\", \"valor\": \"Dor torácica\", \"estado_clinico\": \"+\", \"loc-temp\": \"2015\", \"quem\": \"mãe\"}\n",
    "    ]\n",
    "    \"\"\").strip()\n",
    "\n",
    "    user_input = textwrap.dedent(f\"Texto para Análise:\\n{text}\").strip()\n",
    "    prompt = f\"{instructions}\\\\n\\\\n{user_input}\\\\n\\\\nLista JSON de Entidades Identificadas:\\\\n\"\n",
    "\n",
    "    print(\"\\\\n--- Saída do Modelo (Streaming) ---\")\n",
    "    try:\n",
    "        if USE_LLAMA_CPP:\n",
    "            response_stream = model.create_completion(\n",
    "                prompt=prompt, max_tokens=MAX_TOKENS, temperature=TEMPERATURE,\n",
    "                top_p=TOP_P, top_k=TOP_K, stream=True, min_p=MIN_P,\n",
    "            )\n",
    "            for chunk in response_stream:\n",
    "                if delta := chunk['choices'][0].get('text'):\n",
    "                    print(delta, end=\"\", flush=True)\n",
    "        else:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "            _ = model.generate(**inputs, max_new_tokens=MAX_TOKENS, temperature=TEMPERATURE,\n",
    "                               top_p=TOP_P, top_k=TOP_K, streamer=streamer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n[Erro na geração] {str(e)}\")\n",
    "    finally:\n",
    "        print(\"\\\\n--- Fim da Saída do Modelo ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d9c57",
   "metadata": {},
   "source": [
    "### Função de Carregamento e Processamento de Ficheiros\n",
    "A função load_and_process_file gere a leitura do ficheiro de entrada que contém os diários clínicos. Ela verifica se o ficheiro existe e, em seguida, lê-o linha por linha. Utilizando uma expressão regular (re.compile), a função consegue identificar o início de cada nova entrada no diário (delimitada por um número seguido de um hífen). Ao detetar uma nova entrada, processa a secção anterior acumulada, enviando-a para a função generate_ner_json_streaming para extração das entidades. Este método permite processar ficheiros com múltiplos registos de forma sequencial e organizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função de Carregamento e Processamento de Ficheiros ---\n",
    "def load_and_process_file(file_name: str, model, tokenizer):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"Erro: Ficheiro de entrada '{file_name}' não encontrado.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\\\nProcessando o ficheiro: {file_name}\")\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        entry_index = 0\n",
    "        current_section = \"\"\n",
    "        entry_header_pattern = re.compile(r\"^(\\\\d+)\\\\s*-\")\n",
    "\n",
    "        for line in file:\n",
    "            match = entry_header_pattern.match(line)\n",
    "            if match:\n",
    "                if current_section.strip():\n",
    "                    print(f\"\\\\n\\\\n=== Processando Entrada {entry_index} ===\")\n",
    "                    generate_ner_json_streaming(current_section.strip(), model, tokenizer)\n",
    "                current_section = line\n",
    "                entry_index = int(match.group(1))\n",
    "            else:\n",
    "                current_section += line\n",
    "\n",
    "        if current_section.strip():\n",
    "            print(f\"\\\\n\\\\n=== Processando Entrada {entry_index} ===\")\n",
    "            generate_ner_json_streaming(current_section.strip(), model, tokenizer)\n",
    "\n",
    "    print(\"\\\\n\\\\n=== Processamento Concluído ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6dd65",
   "metadata": {},
   "source": [
    "### Bloco de Execução Principal\n",
    "Este é o ponto de entrada do script. A sua execução está protegida pela condição if __name__ == \"__main__\", uma prática recomendada em Python. O bloco orquestra todo o fluxo de trabalho: primeiro, chama a função initialize_model para carregar o modelo e o tokenizer. Se o modelo for carregado com sucesso, define o nome do ficheiro de entrada (diarios-cardiologia-amostra.txt) e invoca a função load_and_process_file para iniciar a análise. Por fim, após o processamento de todos os ficheiros, executa uma coleta de lixo (gc.collect()) para libertar memória, uma etapa especialmente importante ao lidar com modelos de grande dimensão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_Me1YwmJ32jX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A inicializar o modelo com o backend: llama-cpp-python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\OneDrive\\Ambiente de Trabalho\\Prologica\\Prologica_new\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Llama.cpp inicializado com sucesso.\n",
      "\\nProcessando o ficheiro: diarios-cardiologia-amostra.txt\n",
      "\\n\\n=== Processando Entrada 0 ===\n",
      "\\n--- Saída do Modelo (Streaming) ---\n",
      "```json\n",
      "[\n",
      "  {\"id\": 1, \"classe\": \"S\", \"valor\": \"Dor torácica\"},\n",
      "  {\"id\": 2, \"classe\": \"SV\", \"valor\": \"FC- 57 bpm\"},\n",
      "  {\"id\": 3, \"classe\": \"SV\", \"valor\": \"FC- 72 bpm\"},\n",
      "  {\"id\": 4, \"classe\": \"SV\", \"valor\": \"FC- 61 bpm\"},\n",
      "  {\"id\": 5, \"classe\": \"SV\", \"valor\": \"TA 133/72\"},\n",
      "  {\"id\": 6, \"classe\": \"D\", \"valor\": \"BAV 1º\"},\n",
      "  {\"id\": 7, \"classe\": \"S\", \"valor\": \"cansaço\"},\n",
      "  {\"id\": 8, \"classe\": \"D\", \"valor\": \"Miocardiopatia dilatada\"},\n",
      "  {\"id\": 9, \"classe\": \"SV\", \"valor\": \"VE-47 mm\"},\n",
      "  {\"id\": 10, \"classe\": \"SV\", \"valor\": \"SIV-7mm\"},\n",
      "  {\"id\": 11, \"classe\": \"SV\", \"valor\": \"PP-6 mm\"},\n",
      "  {\"id\": 12, \"classe\": \"SV\", \"valor\": \"Fs 34%\"},\n",
      "  {\"id\": 13, \"classe\": \"SV\", \"valor\": \"TAPSE- 24mm\"},\n",
      "  {\"id\": 14, \"classe\": \"D\", \"valor\": \"IC\"},\n",
      "  {\"id\": 15, \"classe\": \"D\", \"valor\": \"MCD\"},\n",
      "  {\"id\": 16, \"classe\": \"M\", \"valor\": \"dapa\"},\n",
      "  {\"id\": 17, \"classe\": \"M\", \"valor\": \"bisoprolol\"},\n",
      "  {\"id\": 18, \"classe\": \"M\", \"valor\": \"espironolactona\"},\n",
      "  {\"id\": 19, \"classe\": \"M\", \"valor\": \"rosuvastatina\"},\n",
      "  {\"id\": 20, \"classe\": \"M\", \"valor\": \"desosgestrel\"},\n",
      "  {\"id\": 21, \"classe\": \"SV\", \"valor\": \"NYHA II-III\"},\n",
      "  {\"id\": 22, \"classe\": \"M\", \"valor\": \"amlodipina\"},\n",
      "  {\"id\": 23, \"classe\": \"M\", \"valor\": \"Irb/HTZ\"},\n",
      "  {\"id\": 24, \"classe\": \"M\", \"valor\": \"Met/Vilda\"},\n",
      "  {\"id\": 25, \"classe\": \"D\", \"valor\": \"DM 2\"},\n",
      "  {\"id\": 26, \"classe\": \"D\", \"valor\": \"HTA\"},\n",
      "  {\"id\": 27, \"classe\": \"S\", \"valor\": \"picada precordial\"},\n",
      "  {\"id\": 28, \"classe\": \"SV\", \"valor\": \"RS FC 61\"},\n",
      "  {\"id\": 29, \"classe\": \"SV\", \"valor\": \"T neg V4-V5 e derivações inferiores\"},\n",
      "  {\"id\": 30, \"classe\": \"P\", \"valor\": \"Alta\"},\n",
      "  {\"id\": 31, \"classe\": \"D\", \"valor\": \"Dislipidémia\"},\n",
      "  {\"id\": 32, \"classe\": \"S\", \"valor\": \"Cansaço\"},\n",
      "  {\"id\": 33, \"classe\": \"M\", \"valor\": \"Enalaptril\"},\n",
      "  {\"id\": 34, \"classe\": \"S\", \"valor\": \"Roncopatia\"},\n",
      "  {\"id\": 35, \"classe\": \"S\", \"valor\": \"Hipoacusia\"},\n",
      "  {\"id\": 36, \"classe\": \"M\", \"valor\": \"AAS\"},\n",
      "  {\"id\": 37, \"classe\": \"M\", \"valor\": \"Metformina\"},\n",
      "  {\"id\": 38, \"classe\": \"D\", \"valor\": \"FA\"},\n",
      "  {\"id\": 39, \"classe\": \"SV\", \"valor\": \"PSAP= 55mmHg\"},\n",
      "  {\"id\": 40, \"classe\": \"D\", \"valor\": \"MCD\"},\n",
      "  {\"id\": 41, \"classe\": \"M\", \"valor\": \"Dabigatrano\"},\n",
      "  {\"id\": 42, \"classe\": \"M\", \"valor\": \"alopurinol\"},\n",
      "  {\"id\": 43, \"classe\": \"M\", \"valor\": \"gemfibrozil\"},\n",
      "  {\"id\": 44, \"classe\": \"M\", \"valor\": \"Amiodarona\"},\n",
      "  {\"id\": 45, \"classe\": \"M\", \"valor\": \"Insulina\"},\n",
      "  {\"id\": 46, \"classe\": \"D\", \"valor\": \"Crises gota\"},\n",
      "  {\"id\": 47, \"classe\": \"SV\", \"valor\": \"FC 67 ppm\"},\n",
      "  {\"id\": 48, \"classe\": \"SV\", \"valor\": \"FC 130 ppm\"},\n",
      "  {\"id\": 49, \"classe\": \"SV\", \"valor\": \"FC média 95 ppm\"},\n",
      "  {\"id\": 50, \"classe\": \"SV\", \"valor\": \"PR 120 ms\"},\n",
      "  {\"id\": 51, \"classe\": \"SV\", \"valor\": \"PR 173-176ms\"},\n",
      "  {\"id\": 52, \"classe\": \"D\", \"valor\": \"HVE\"},\n",
      "  {\"id\": 53, \"classe\": \"SV\", \"valor\": \"FEVE 24%\"},\n",
      "  {\"id\": 54, \"classe\": \"M\", \"valor\": \"dapa\"},\n",
      "  {\"id\": 55, \"classe\": \"M\", \"valor\": \"bisoprolol\"},\n",
      "  {\"id\": 56, \"classe\": \"SV\", \"valor\": \"TA 120/70\"},\n",
      "  {\"id\": 57, \"classe\": \"D\", \"valor\": \"HBP\"},\n",
      "  {\"id\": 58, \"classe\": \"M\", \"valor\": \"furosemida\"},\n",
      "  {\"id\": 59, \"classe\": \"D\", \"valor\": \"SAOS\"},\n",
      "  {\"id\": 60, \"classe\": \"M\", \"valor\": \"AAS\"},\n",
      "  {\"id\": 61, \"classe\": \"S\", \"valor\": \"palpitações\"},\n",
      "  {\"id\": 62, \"classe\": \"S\", \"valor\": \"picada\"},\n",
      "  {\"id\": 63, \"classe\": \"SV\", \"valor\": \"AP sem estase\"},\n",
      "  {\"id\": 64, \"classe\": \"S\", \"valor\": \"precordialgia\"},\n",
      "  {\"id\": 65, \"classe\": \"P\", \"valor\": \"Rever 1 ano\"},\n",
      "  {\"id\": 66, \"classe\": \"P\", \"valor\": \"Alta\"},\n",
      "  {\"id\": 67, \"classe\": \"D\", \"valor\": \"Trissomia 21\"},\n",
      "  {\"id\": 68, \"classe\": \"D\", \"valor\": \"Hipotiroidismo\"},\n",
      "  {\"id\": 69, \"classe\": \"D\", \"valor\": \"Obesidade\"},\n",
      "  {\"id\": 70, \"classe\": \"S\", \"valor\": \"roncopatia\"},\n",
      "  {\"id\": 71, \"classe\": \"S\", \"valor\": \"Surdez bilateral grave\"},\n",
      "  {\"id\": 72, \"classe\": \"S\", \"valor\": \"Piodermite recidivante\"},\n",
      "  {\"id\": 73, \"classe\": \"P\", \"valor\": \"retomar aulas\"},\n",
      "  {\"id\": 74, \"classe\": \"S\", \"valor\": \"cansço\"},\n",
      "  {\"id\": 75, \"classe\": \"S\", \"valor\": \"ortopneia\"},\n",
      "  {\"id\": 76, \"classe\": \"S\", \"valor\": \"DPN\"},\n",
      "  {\"id\": 77, \"classe\": \"SV\", \"valor\": \"P 70\"}\n",
      "]\n",
      "```\\n--- Fim da Saída do Modelo ---\n",
      "\\n\\n=== Processamento Concluído ===\n",
      "\\nExecutando a coleta de lixo final...\n",
      "Script finalizado.\n"
     ]
    }
   ],
   "source": [
    "# --- Bloco de Execução Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = initialize_model()\n",
    "\n",
    "    if model:\n",
    "        INPUT_FILE = \"diarios-cardiologia-amostra.txt\"\n",
    "        load_and_process_file(INPUT_FILE, model, tokenizer)\n",
    "\n",
    "    print(\"\\\\nExecutando a coleta de lixo final...\")\n",
    "    gc.collect()\n",
    "    if not USE_LLAMA_CPP and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Script finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Prologica_new (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
